{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "情感分析"
    ]
   },
   "source": [
    "情感分析\n",
    "=\n",
    "模型从简单到复杂，依次构建：\n",
    "-\n",
    "* Word Averaging 模型\n",
    "* RNN/LSTM 模型\n",
    "* CNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True # why??????\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbere of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Numbere of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['While', 'Urban', 'Cowboy', 'did', 'not', 'ooze', 'with', 'the', 'same', 'testosterone', 'you', 'might', 'find', 'at', 'a', 'rodeo', ',', 'it', 'did', 'provide', 'an', 'accurate', 'glimpse', 'of', 'that', 'day', 'and', 'age', ',', 'in', 'urban', 'Texas', '.', 'I', 'also', 'think', 'that', 'to', 'truly', 'critique', 'this', 'movie', ',', 'one', 'would', 'have', 'to', 'have', 'lived', 'in', 'the', 'time', 'and', 'relative', 'place', 'that', 'it', 'was', 'made', '.', 'There', 'was', 'good', 'music', ',', 'fun', 'times', 'and', ',', 'yes', ',', 'a', 'few', '\"', 'rough', 'and', 'tumbles', '\"', 'at', 'the', 'honky', 'tonk', 'roadhouses', '.', 'The', 'relationship', 'of', 'Bud', 'and', 'Sissy', ',', 'like', '\"', 'two', 'ships', 'passing', 'in', 'the', 'night', '\"', ',', 'was', 'well', 'conceived', '.', 'When', 'Pam', 'tore', 'up', 'the', 'note', 'that', 'Sissy', 'had', 'written', 'to', 'Bud', ',', 'it', 'echoed', 'the', 'tragedy', 'of', 'many', 'true', 'life', 'romances', '.', 'The', 'entire', 'story', 'was', 'well', 'thought', 'out', '.', 'I', 'thought', 'the', 'cast', 'and', 'crew', 'did', 'an', 'excellent', 'job', '.', 'I', 'thought', 'the', 'screen', 'play', 'was', 'well', 'written', 'and', 'directed', '.', 'Scott', 'Glenn', 'should', 'have', 'received', 'an', 'Oscar', 'for', 'best', 'supporting', 'actor', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下每部分有多少数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下一步我们需要创建 vocabulary . vocabulary 就是把每个单词一一映射到一个数字 one hot\n",
    "* 我们使用最常见的25k个单词来构建我们的单词表， 用Max_size这个参数可以做到这一点\n",
    "* 所有其他的单词都用<unk>来表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义循环以及各种参数\n",
    "-\n",
    "note: BucketIterator 还表示会把vocabulary进行大小的顺序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, pad_idx):\n",
    "        super(WordAVGModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx) # 在embed中把padding的词向量初始化为0\n",
    "        self.linear = nn.Linear(embedding_size, output_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embed(text) # [seq_len, batch_size, embedding_size]\n",
    "#         embedded = embedded.transpose(1, 0 ) # [batch_size, seq_len, embedding_size]\n",
    "        embedded = embedded.permute(1, 0 ,2) # [batch_size, seq_len, embedding_size] permute即为重排序\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)) # [batch_size, 1 , embedding_size] embedded.shape[1]表示这个维度全压扁， 1 表示根本不压\n",
    "        pooled = pooled.squeeze() # [batch_size, embedding_size]\n",
    "        return self.linear(pooled)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordAVGModel(vocab_size = VOCAB_SIZE,\n",
    "                     embedding_size = EMBEDDING_SIZE,\n",
    "                     output_size = OUTPUT_SIZE,\n",
    "                     pad_idx = PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum( p.numel() for p in model.parameters() if p.requires_grad) #numel 数一共有多少参数\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型词向量的初始化，利用stanford的glove.6B.100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "# pretrained_embedding.shape\n",
    "model.embed.weight.data.copy_(pretrained_embedding)  # 把embed.weight.data 初始化为 pretrained_embedding 的形式\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss() # 需要学习各种criterion\n",
    "\n",
    "model = model.to(device)\n",
    "crierion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds)) # 四舍五入为 0 1 的数\n",
    "    correct = (rounded_preds == y).float() # rounded_preds ==y 返回的是 True False, 后面加上 .float() 就会变成0 1\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.train()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = criterion(preds, batch.label)\n",
    "        acc = binary_accuracy(preds, batch.label)\n",
    "        \n",
    "        #sgd\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "        \n",
    "    return epoch_loss / total_len, epoch_acc/ total_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.eval()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = criterion(preds, batch.label)\n",
    "        acc = binary_accuracy(preds, batch.label)\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.5512726911408561 Train Acc 0.7856571429116386\n",
      "Epoch 0 Valid Loss 0.4839723803520203 Valid Acc 0.8262666666984558\n",
      "Epoch 1 Train Loss 0.4278651227542332 Train Acc 0.8459428571973528\n",
      "Epoch 1 Valid Loss 0.39040902093251545 Valid Acc 0.8608000000317891\n",
      "Epoch 2 Train Loss 0.34556904196739197 Train Acc 0.876685714326586\n",
      "Epoch 2 Valid Loss 0.33838990990320844 Valid Acc 0.8756000000317892\n",
      "Epoch 3 Train Loss 0.2923483277797699 Train Acc 0.89828571434021\n",
      "Epoch 3 Valid Loss 0.30775027653376263 Valid Acc 0.8817333333651225\n",
      "Epoch 4 Train Loss 0.2545447506427765 Train Acc 0.9115428572382246\n",
      "Epoch 4 Valid Loss 0.28786284338633217 Valid Acc 0.8886666666984558\n",
      "Epoch 5 Train Loss 0.2248978919846671 Train Acc 0.9250285714558193\n",
      "Epoch 5 Valid Loss 0.2757629540920258 Valid Acc 0.8918666666984558\n",
      "Epoch 6 Train Loss 0.20098931844575066 Train Acc 0.9345142857960292\n",
      "Epoch 6 Valid Loss 0.26628181886672975 Valid Acc 0.8934666666984558\n",
      "Epoch 7 Train Loss 0.18019415139470782 Train Acc 0.94188571434021\n",
      "Epoch 7 Valid Loss 0.2603749723434448 Valid Acc 0.8944000000317891\n",
      "Epoch 8 Train Loss 0.1621374768120902 Train Acc 0.9495428571973528\n",
      "Epoch 8 Valid Loss 0.25641553325653077 Valid Acc 0.8960000000317891\n",
      "Epoch 9 Train Loss 0.14637829198156085 Train Acc 0.9568000000817435\n",
      "Epoch 9 Valid Loss 0.2542413857460022 Valid Acc 0.8956000000317892\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"wordavg-model.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"wordavg-model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [ tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device) # [seq_len]\n",
    "    tensor = tensor.unsqueeze(1) # [seq_len, batch_size] batch_size = 1\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999974966049194"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is dangerous!\") # the result is very weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordAVG结束\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN模型\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, hidden_size, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, bidirectional = True, num_layers = 2)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embed(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded)   # output是所有hidden state， hidden是最后一层的hidden state\n",
    "        \n",
    "        # hidden: [2, batch_size, hidden_size]\n",
    "        hidden = torch.cat([hidden[-1],hidden[-2]],dim = 1) # why?????\n",
    "        hidden = self.dropout(hidden.squeeze())\n",
    "        return self.linear(hidden)\n",
    "        \n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size = VOCAB_SIZE,\n",
    "                 embedding_size = EMBEDDING_SIZE,\n",
    "                 output_size = OUTPUT_SIZE,\n",
    "                 pad_idx = PAD_IDX,\n",
    "                 hidden_size = 100,\n",
    "                 dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "# pretrained_embedding.shape\n",
    "model.embed.weight.data.copy_(pretrained_embedding)  # 把embed.weight.data 初始化为 pretrained_embedding 的形式\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss() # 需要学习各种criterion\n",
    "\n",
    "model = model.to(device)\n",
    "crierion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.6586226647921971 Train Acc 0.597371428666796\n",
      "Epoch 0 Valid Loss 0.6078889149983724 Valid Acc 0.6725333333651224\n",
      "Epoch 1 Train Loss 0.597791870376042 Train Acc 0.6787428572246007\n",
      "Epoch 1 Valid Loss 0.48876981029510497 Valid Acc 0.7682666666666667\n",
      "Epoch 2 Train Loss 0.5519022122383118 Train Acc 0.7268000000953674\n",
      "Epoch 2 Valid Loss 0.61093304438591 Valid Acc 0.6617333333333333\n",
      "Epoch 3 Train Loss 0.4441910290036883 Train Acc 0.8013142857824053\n",
      "Epoch 3 Valid Loss 0.42243079606691997 Valid Acc 0.799066666730245\n",
      "Epoch 4 Train Loss 0.4548588158743722 Train Acc 0.7856000000681196\n",
      "Epoch 4 Valid Loss 0.3499969251314799 Valid Acc 0.8554666666984558\n",
      "Epoch 5 Train Loss 0.31490092885153637 Train Acc 0.8705142857687814\n",
      "Epoch 5 Valid Loss 0.29885362601280213 Valid Acc 0.8794666666984559\n",
      "Epoch 6 Train Loss 0.2671865736280169 Train Acc 0.8939428571837289\n",
      "Epoch 6 Valid Loss 0.3194438885132472 Valid Acc 0.8702666666666666\n",
      "Epoch 7 Train Loss 0.28326265663419453 Train Acc 0.8911428571837289\n",
      "Epoch 7 Valid Loss 0.2855745978116989 Valid Acc 0.8848\n",
      "Epoch 8 Train Loss 0.2076864941392626 Train Acc 0.9217714286531721\n",
      "Epoch 8 Valid Loss 0.27720087593396503 Valid Acc 0.8928\n",
      "Epoch 9 Train Loss 0.17675897523675646 Train Acc 0.934742857170105\n",
      "Epoch 9 Valid Loss 0.28622853958209354 Valid Acc 0.8961333333333333\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"lstm-model.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN模型\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoon Kim, Convolutional Neural Networks for Sentence Classification, 2014\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义的这个模型是3个filter_size不同的CNN模型，理论上K折检验可以直接用这种方法写出来\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_sizes, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels = 1, out_channels = num_filters, \n",
    "                      kernel_size = (fs, embedding_size)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "#         self.conv = nn.Conv2d(in_channels = 1, out_channels = num_filters, kernel_size = (filter_size, embedding_size))\n",
    "        self.linear = nn.Linear(num_filters * len(filter_sizes), output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1,0) # [batch_size, seq_len]\n",
    "        embedded = self.embed(text) # [batch_size ,seq_len, embedding_size]\n",
    "        embedded = embedded.unsqueeze(1) # [batch_size, 1, seq_len, embedding_size]\n",
    "#         conved = F.relu(self.conv(embedded)) # [batch_size, num_filters, seq_len - filter_size +1 , 1]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] \n",
    "#         conved = conved.squeeze(3) # [batch_size, num_filters, seq_len - filter_size +1 ]\n",
    "        # max over time pooling\n",
    "#         pooled = F.max_pool1d(conved, conved.shape[2]) # [batch_size, num_filters, 1]\n",
    "#         pooled = pooled.squeeze(2)\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        pooled = torch.cat(pooled, dim = 1) # [batch_size, 3*num_filters]\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(vocab_size = VOCAB_SIZE,\n",
    "            embedding_size = EMBEDDING_SIZE,\n",
    "            output_size = OUTPUT_SIZE,\n",
    "            pad_idx = PAD_IDX,\n",
    "            num_filters = 100, \n",
    "            filter_sizes =[3, 4, 5],\n",
    "            dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "# pretrained_embedding.shape\n",
    "model.embed.weight.data.copy_(pretrained_embedding)  # 把embed.weight.data 初始化为 pretrained_embedding 的形式\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss() # 需要学习各种criterion\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.6529268033300127 Train Acc 0.610971428666796\n",
      "Epoch 0 Valid Loss 0.5137638531684875 Valid Acc 0.7816\n",
      "Epoch 1 Train Loss 0.43114568466459 Train Acc 0.797485714326586\n",
      "Epoch 1 Valid Loss 0.35205441596508025 Valid Acc 0.8484\n",
      "Epoch 2 Train Loss 0.3050284328392574 Train Acc 0.8746857143947057\n",
      "Epoch 2 Valid Loss 0.3147250477353732 Valid Acc 0.8672\n",
      "Epoch 3 Train Loss 0.22456137347902572 Train Acc 0.9123428572109767\n",
      "Epoch 3 Valid Loss 0.3078390346844991 Valid Acc 0.8684000000317892\n",
      "Epoch 4 Train Loss 0.16277963508878435 Train Acc 0.9375428572518485\n",
      "Epoch 4 Valid Loss 0.30748442580302554 Valid Acc 0.8754666666666666\n",
      "Epoch 5 Train Loss 0.11796935871669224 Train Acc 0.9583428572518485\n",
      "Epoch 5 Valid Loss 0.3260279593884945 Valid Acc 0.8728\n",
      "Epoch 6 Train Loss 0.08232758175475256 Train Acc 0.9712000000272478\n",
      "Epoch 6 Valid Loss 0.3485170791953802 Valid Acc 0.8741333333333333\n",
      "Epoch 7 Train Loss 0.054858031742913385 Train Acc 0.9836000000272478\n",
      "Epoch 7 Valid Loss 0.37702733241418995 Valid Acc 0.8746666666666667\n",
      "Epoch 8 Train Loss 0.04370886615855353 Train Acc 0.9862285714558192\n",
      "Epoch 8 Valid Loss 0.4147105340719223 Valid Acc 0.8721333333333333\n",
      "Epoch 9 Train Loss 0.030136179567873476 Train Acc 0.9908\n",
      "Epoch 9 Valid Loss 0.43825415618469316 Valid Acc 0.8721333333333333\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), \"cnn-model.pth\")\n",
    "        \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
