{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "## 学习目标\n",
    "* 学习语言模型\n",
    "* 学习torchtext的基本使用方法\n",
    "    * 构建vocabulary\n",
    "    * word_to_index and index_to_word\n",
    "* 学习torch.nn的一些基本模型\n",
    "    * Linear\n",
    "    * RNN\n",
    "    * LSTM\n",
    "    * GRU\n",
    "* RNN的训练技巧\n",
    "    * Gradient Clipping\n",
    "* 如何保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先使用torchtext来创建vocabulary。然后把数据读成batch的格式。可以去github自行阅读torchtext的readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3'\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1234)\n",
    "    \n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(USE_CUDA)\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 继续使用text8作为训练验证测试\n",
    "* torchtext 的一个重要的概念是Field，它决定数据如何被处理。吃用TEXT这个Field来处理文本数据。 其中有lower = True 这个参数，即所有的单词都会被lowercase。\n",
    "* torchtext提供了LanguageModelingDataset这个class来帮助处理语言模型数据集。\n",
    "* build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量\n",
    "* BPTTiterator可以连续地得到连贯的句子，BPTT全称为back propagation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower = True)\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path = \".\",\n",
    "                            train = \"text8.train.txt\", validation = \"text8.dev.txt\",\n",
    "                            test = \"text8.test.txt\", text_field = TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 之所以是50002是因为torchtext增加了两个特殊的token，<unk>表示unknown的单词，<pad>表示padding\n",
    "* 模型的输入是一串文字，模型的输出也是遗传文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.vocab.stoi[\"damn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter , val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size = BATCH_SIZE, device=device, \n",
    "    bptt_len = 100, repeat = False, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in enumerate(train_iter):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 100x64 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 100x64 (GPU 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:, 0].data.cpu()))\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:, 0].data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义模型\n",
    "="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, text, hidden):\n",
    "        # forward pass\n",
    "        # text: [seq_length, batch_size]\n",
    "        emb = self.embed(text) # emb: [seq_length, batch_size, embed_size]\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        # output: [seq_length, batch_size, hidden_size]\n",
    "        # hidden: ([1(number of layers etc. bert), batch_size, hidden_size], [1(number of layers etc. bert) , batch_size, hidden_size])\n",
    "        out_vocab = self.linear(output.view(-1, output.shape[2])) # [seq_length*batch_size, hidden_size] 这里是因为output是三维的，而linear需要输入是二维的，所以用view压成二维的函数 \n",
    "        out_vocab = out_vocab.view(output.size(0), output.size(1), out_vocab.size(-1)) # 这里再重新释放出来\n",
    "        # 这里不用加sigmoid之类的activation，因为本身lstm的gate就扮演了这样的角色 ,当然也可以加，只是默认不加\n",
    "        return out_vocab, hidden\n",
    "        \n",
    "    def init_hidden(self, batchsize, requires_grad = True):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, batchsize, self.hidden_size), requires_grad = True),\n",
    "                weight.new_zeros((1, batchsize, self.hidden_size), requires_grad = True))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个模型\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size = len(TEXT.vocab), \n",
    "                 embed_size = EMBEDDING_SIZE,\n",
    "                 hidden_size = HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    momdel = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.5903, -0.1947, -0.2415],\n",
       "        [ 1.3204,  1.5997, -1.0792,  ...,  0.6060,  0.2209, -0.8245],\n",
       "        [ 0.7289, -0.7336,  1.5624,  ..., -0.5592, -0.4480, -0.6476],\n",
       "        ...,\n",
       "        [ 0.5675,  1.4622, -0.5770,  ..., -0.4970, -0.3513,  1.9668],\n",
       "        [-0.2747,  1.3695, -0.5266,  ..., -1.2115,  0.1327, -0.7934],\n",
       "        [ 1.4297, -0.1843,  0.2579,  ..., -0.0684,  0.5642,  0.6348]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来训练模型：\n",
    "-\n",
    "* 若干epoch\n",
    "* 每个epoch分成若干batch\n",
    "* 把每个batch输入输出的数据包装为 cuda tensor\n",
    "* forward pass ，通过输入的句子预测每个单词的下个单词\n",
    "* 用模型的预测和正确的下个单词计算cross entropy loss\n",
    "* 清空模型当前gradient\n",
    "* backward pass\n",
    "* gradient clipping，防止梯度爆炸\n",
    "* 更新模型参数\n",
    "* 每隔一定的iteration输出模型当前loss，以及在验证集上做模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)    # 这个h是一个全新的开始，没有把历史保存下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_count = 0.\n",
    "    it = iter(data)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad = False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_loss = loss.item() * np.multiply(*data.size())\n",
    "            total_count = np.multiply(*data.size())\n",
    "            \n",
    "    loss = total_loss/total_count\n",
    "    model.train()   #这一步很有必要\n",
    "    return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iterator 0        |     loss 6.5254669189453125\n",
      "epoch 0 iterator 0       |    loss of validation 6.448461055755615\n",
      "best model saved to language_model3.pth\n",
      "epoch 0 iterator 100        |     loss 6.348505020141602\n",
      "epoch 0 iterator 200        |     loss 6.3520002365112305\n",
      "epoch 0 iterator 300        |     loss 6.081464767456055\n",
      "epoch 0 iterator 400        |     loss 6.179876327514648\n",
      "epoch 0 iterator 500        |     loss 6.229490756988525\n",
      "epoch 0 iterator 600        |     loss 6.128426551818848\n",
      "epoch 0 iterator 700        |     loss 6.121902942657471\n",
      "epoch 0 iterator 800        |     loss 6.201119422912598\n",
      "epoch 0 iterator 900        |     loss 6.033709526062012\n",
      "epoch 0 iterator 1000        |     loss 6.113443374633789\n",
      "epoch 0 iterator 1000       |    loss of validation 6.225577354431152\n",
      "best model saved to language_model3.pth\n",
      "epoch 0 iterator 1100        |     loss 6.0718231201171875\n",
      "epoch 0 iterator 1200        |     loss 6.109519004821777\n",
      "epoch 0 iterator 1300        |     loss 5.9593915939331055\n",
      "epoch 0 iterator 1400        |     loss 6.183826923370361\n",
      "epoch 0 iterator 1500        |     loss 5.963066577911377\n",
      "epoch 0 iterator 1600        |     loss 6.0393452644348145\n",
      "epoch 0 iterator 1700        |     loss 5.9136552810668945\n",
      "epoch 0 iterator 1800        |     loss 6.052790641784668\n",
      "epoch 0 iterator 1900        |     loss 6.077558517456055\n",
      "epoch 0 iterator 2000        |     loss 5.967501640319824\n",
      "epoch 0 iterator 2000       |    loss of validation 6.033618927001953\n",
      "best model saved to language_model3.pth\n",
      "epoch 0 iterator 2100        |     loss 5.828682899475098\n",
      "epoch 0 iterator 2200        |     loss 5.885374546051025\n",
      "epoch 0 iterator 2300        |     loss 5.8916754722595215\n",
      "epoch 1 iterator 0        |     loss 6.017367362976074\n",
      "epoch 1 iterator 0       |    loss of validation 5.966148853302002\n",
      "best model saved to language_model3.pth\n",
      "epoch 1 iterator 100        |     loss 5.917176723480225\n",
      "epoch 1 iterator 200        |     loss 5.93604850769043\n",
      "epoch 1 iterator 300        |     loss 5.648369312286377\n",
      "epoch 1 iterator 400        |     loss 5.821188926696777\n",
      "epoch 1 iterator 500        |     loss 5.8603315353393555\n",
      "epoch 1 iterator 600        |     loss 5.779733657836914\n",
      "epoch 1 iterator 700        |     loss 5.7266387939453125\n",
      "epoch 1 iterator 800        |     loss 5.867396831512451\n",
      "epoch 1 iterator 900        |     loss 5.689525127410889\n",
      "epoch 1 iterator 1000        |     loss 5.793387413024902\n",
      "epoch 1 iterator 1000       |    loss of validation 5.833555221557617\n",
      "best model saved to language_model3.pth\n",
      "epoch 1 iterator 1100        |     loss 5.760354518890381\n",
      "epoch 1 iterator 1200        |     loss 5.761814117431641\n",
      "epoch 1 iterator 1300        |     loss 5.632013320922852\n",
      "epoch 1 iterator 1400        |     loss 5.845232963562012\n",
      "epoch 1 iterator 1500        |     loss 5.6264448165893555\n",
      "epoch 1 iterator 1600        |     loss 5.750026702880859\n",
      "epoch 1 iterator 1700        |     loss 5.581701755523682\n",
      "epoch 1 iterator 1800        |     loss 5.768184661865234\n",
      "epoch 1 iterator 1900        |     loss 5.759565830230713\n",
      "epoch 1 iterator 2000        |     loss 5.634308338165283\n",
      "epoch 1 iterator 2000       |    loss of validation 5.731061935424805\n",
      "best model saved to language_model3.pth\n",
      "epoch 1 iterator 2100        |     loss 5.5362935066223145\n",
      "epoch 1 iterator 2200        |     loss 5.563508987426758\n",
      "epoch 1 iterator 2300        |     loss 5.617062568664551\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_EPOCHS = 2\n",
    "GRAD_CLIP = 5.\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden) # problem is : backpropogate through all iterations  计算很大很深，并且只有语言模型才会这样传hidden，翻译也不会这么做\n",
    "        \n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "        # \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0 :\n",
    "            print(\"epoch\",epoch,\"iterator\",i,\"       |     loss\", loss.item())\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            print(\"epoch\",epoch,\"iterator\",i,\"      |    loss of validation\" , val_loss)\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model saved to language_model3.pth\")\n",
    "                torch.save(model.state_dict(),\"languagem_model3.pth\")\n",
    "            else:\n",
    "                # learning rate decay\n",
    "                scheduler.step()\n",
    "                \n",
    "            val.losses.append(val_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取模型\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RNNModel(vocab_size = len(TEXT.vocab), \n",
    "                 embed_size = EMBEDDING_SIZE,\n",
    "                 hidden_size = HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    best_momdel = best_model.to(device)\n",
    "    \n",
    "best_model.load_state_dict(torch.load(\"languagem_model3.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用最好的模型在valid数据上计算perplexity\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity :  308.2964855642628\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(best_model , val_iter)\n",
    "print(\"perplexity : \", np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的的模型来生成一些句子\n",
    "-\n",
    "很重要，多看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maronites chamorros korea ecps omelette defeats paperback impressions <unk> spaceflight two nine eight gott f n zero as six zero one nicolson two two zero three g himmel cook and trail slide crimson quotas see a standard numeral for several diameters state or concretes the atari tnt judges in the town six th edition in music was stone dressing but most notably guinness style of bologna points and that admire the air marker the batter still exist the conventional color for workflow during the beatles writing at each of contents that the great majority of alpha is now surprising by\n"
     ]
    }
   ],
   "source": [
    "hidden = best_model.init_hidden(1)  # 拿一个batch_size是 1 的hidden state\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype = torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    # run forward pass\n",
    "    output, hidden = best_model(input, hidden)\n",
    "    # logits exp (like softmax)\n",
    "    word_weights = output.squeeze().exp().cpu() #squeeze（）把所有维度为 1 的部分扔掉\n",
    "    # multinomial sampling : the function gets the index of input instead of the value of the input\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    # fill in the current predicted word to the current input\n",
    "    input.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "    \n",
    "print(\" \".join(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
